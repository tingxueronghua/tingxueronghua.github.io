<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yucheng Han</title>

    <meta name="author" content="Yucheng Han">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yucheng Han
                </p>
                <p>I graduated from Tsinghua University in 2021. Now I am a fourth year Ph.D. student at Nanyang Technological University advised by Professor <a href="https://personal.ntu.edu.sg/hanwangzhang/" target="_blank" rel="noopener">Hanwang Zhang</a>. Recently I work as an intern advised by <a href="https://www.skicyyu.org/">Gang Yu</a>. </p>
                </p>
                <p>I mainly focus on computer vision. I can be contacted using email <a href="mailto:yucheng002@e.ntu.edu.sg">yucheng002@e.ntu.edu.sg</a>.</p>
                <p style="text-align:center">
                  <a href="yucheng002@e.ntu.edu.sg">Email</a> &nbsp;/&nbsp;
                  <a href="images/resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=LbwqJBQAAAAJ&hl=zh-CN">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/yucheng_han">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/tingxueronghua">Github</a>
                </p>
              </td>
              <td class="image">
                <a href="images/personal_image.jpg"><img alt="profile photo" src="images/personal_image.jpg" class="hoverZoomLink"></a>
              </td>
              
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in generative AI recently. I have some experiences in Image Editing, Multi-modal Large Language Model, 3D object detection, prompt learning, and video summarization. </span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    
            <!-- <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()" bgcolor="#ffffd0"> -->
            <tr>
              <td class="paper-image"><img src="https://github.com/stepfun-ai/Step1X-Edit/blob/main/assets/results_show.png"></td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://github.com/stepfun-ai/Step1X-Edit">
              <span class="papertitle">Step1X-Edit: A Practical Framework for General Image Editing
        </span>
                </a>
                <br>
                Shiyu Liu, 
                <strong>Yucheng Han</strong>, 
                Peng Xing, 
                Fukun Yin, 
                Rui Wang, 
                Wei Cheng, 
                Jiaqi Liao, 
                Yingming Wang, 
                Honghao Fu, 
                Chunrui Han, 
                Guopeng Li, 
                Yuang Peng, 
                Quan Sun, 
                Jingwei Wu, 
                Yan Cai, 
                Zheng Ge, 
                Ranchen Ming, 
                Lei Xia, 
                Xianfang Zeng, 
                Yibo Zhu, 
                Binxing Jiao, 
                Xiangyu Zhang, 
                Gang Yu, 
                Daxin Jiang
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://github.com/stepfun-ai/Step1X-Edit">project page</a>
                /
                <a href="https://arxiv.org/abs/2504.17761">arXiv</a>
                <p></p>
                <p>
                  Combining the most recent VLM and in-house DiT, we open-source an image-editing model that could compare with closed-source image-editing models.
                </p>
              </td>
            </tr>
            
            <tr>
              <td class="paper-image"><img src="images/emma.jpg"></td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://tencentqqgylab.github.io/EMMA">
              <span class="papertitle">EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts
        </span>
                </a>
                <br>
                <strong>Yucheng Han*</strong>,
                <a href="https://wrong.wang/">Rui Wang*</a>,
                <a href="https://icoz69.github.io/">Chi Zhang*</a>,
                Juntao Hu,
                Pei Cheng,
                Bin Fu,
                <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://tencentqqgylab.github.io/EMMA">project page</a>
                /
                <a href="https://arxiv.org/abs/2406.09162">arXiv</a>
                <p></p>
                <p>
                  EMMA, a novel model based on ELLA, enhances the capability of multi-modal conditioned image generation by a unique perceiver resampler. It maintains fidelity and detail in generated images, and follows text instructions at the same time, proving an effective solution for diverse multi-modal conditional image generation tasks.
                </p>
              </td>
            </tr>
            <tr>
              <td class="paper-image"><img src="images/appagent.jpg"></td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://appagent-official.github.io/">
              <span class="papertitle">AppAgent: Multimodal Agents as Smartphone Users
        </span>
                </a>
                <br>
                <a href="https://icoz69.github.io/">Chi Zhang</a>*,
                <a href="https://github.com/yz93">Zhao Yang</a>*,
                <a href="https://www.linkedin.com/in/jiaxuan-liu-9051b7105/">Jiaxuan Liu</a>*,
                <strong>Yucheng Han</strong>,
                <a href="https://chenxin.tech/">Xin Chen</a>,
                Zebiao Huang,
                <a href="https://openreview.net/profile?id=~BIN_FU2">Bin Fu</a>,
                <a href="https://www.skicyyu.org/">Gang Yu</a>
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://appagent-official.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2312.13771">arXiv</a>
                <p></p>
                <p>
                  This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. The framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping.
                </p>
              </td>
            </tr>
            <tr>
              <td class="paper-image"><img src="images/chartllama.jpg"></td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://tingxueronghua.github.io/ChartLlama/">
              <span class="papertitle">ChartLlama: A Multimodal LLM for Chart Understanding and Generation
        </span>
                </a>
                <br>
                <strong>Yucheng Han</strong>*,
                <a href="https://icoz69.github.io/">Chi Zhang</a>*,
                <a href="https://chenxin.tech/">Xin Chen</a>,
                <a href="https://cse.seu.edu.cn/2021/1126/c23024a392593/page.htm">Xu Yang</a>,
                <a href="https://openreview.net/profile?id=~Billzb_Wang1">Zhibin Wang</a>,
                <a href="https://www.skicyyu.org/">Gang Yu</a>,
                <a href="https://openreview.net/profile?id=~BIN_FU2">Bin Fu</a>,
                <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://tingxueronghua.github.io/ChartLlama/">project page</a>
                /
                <a href="https://arxiv.org/abs/2311.16483">arXiv</a>
                <p></p>
                <p>
                This paper propose an instruction-following dataset construction method for chart figures and finetune a LLaVA-1.5-13B to comprehend and generate chart figures.
                </p>
              </td>
            </tr>
            <tr>
              <td class="paper-image"><img src="images/dpke.jpg"></td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://github.com/tingxueronghua/DPKE">
              <span class="papertitle">Dual-Perspective Knowledge Enrichment for Semi-Supervised 3D Object Detection
        </span>
                </a>
                <br>
                <strong>Yucheng Han</strong>,
                <a href="https://na-z.github.io/">Na Zhao</a>*,
                Weiling Chen,
                Keng Teck Ma,
                <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>
                <br>
                <em>AAAI</em>, 2024
                <br>
                <a href="https://github.com/tingxueronghua/DPKE">project page</a>
                /
                <a href="https://arxiv.org/abs/2405.10314">arXiv</a>
                <p></p>
                <p>
                  A novel Dual-Perspective Knowledge Enrichment approach named DPKE for semi-supervised 3D object detection. DPKE enriches the knowledge of limited training data, particularly unlabeled data, from data-perspective and feature-perspective. 
                </p>
              </td>
            </tr>
            <tr>
              <td class="paper-image"><img src="images/prograd.jpg"></td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2205.14865">
              <span class="papertitle">Prompt-aligned Gradient for Prompt Tuning
        </span>
                </a>
                <br>
                <a href="https://beierzhu.github.io/">Beier Zhu</a>,
                <a href="https://yuleiniu.github.io/">Yulei Niu</a>,
                <strong>Yucheng Han</strong>,
                Yue Wu,
                <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a>
                <br>
                <em>ICCV</em>, 2023
                <br>
                <a href="https://cat3d.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2405.10314">arXiv</a>
                <p></p>
                <p>
                  We present Prompt-aligned Gradient, dubbed ProGrad, to prevent prompt tuning from forgetting the the general knowledge learned from VLMs. In particular, ProGrad only updates the prompt whose gradient is aligned (or non-conflicting) to the "general direction", which is represented as the gradient of the KL loss of the pre-defined prompt prediction. 
                </p>
              </td>
            </tr>





          
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h2>Miscellanea</h2>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
              <br>
              <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
              <br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
          

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
              <h2>Basically <br> Blog Posts</h2>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
              <br>
              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
              <br>
              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
              <br>
              <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
            </td>
          </tr>
          
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table> -->
</body>
</html>
